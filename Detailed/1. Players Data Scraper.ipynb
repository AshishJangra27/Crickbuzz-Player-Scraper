{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b7eb6f",
   "metadata": {},
   "source": [
    "### 1. Scraping Cricket Teams Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "297107e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teams.csv is created!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data = []\n",
    "\n",
    "soup = BeautifulSoup(requests.get('https://www.cricbuzz.com/cricket-team').content,'html.parser')\n",
    "\n",
    "# Data of all the  Teams\n",
    "sp = soup.find('div', class_ = 'cb-col cb-col-100')                                       \n",
    "\n",
    "# Data of Each Team displayed at Left\n",
    "for team_data in sp.find_all('div', class_ = 'cb-col cb-col-50 cb-team-item cb-lst-itm cb-team-lft-item'):   \n",
    "    team_name = team_data.text.strip()\n",
    "    team_link = 'https://www.cricbuzz.com' + team_data.find('a').get('href') + '/players'\n",
    "    data.append([team_name, team_link])\n",
    "\n",
    "# Data of Each Team displayed at Right\n",
    "for team_data in sp.find_all('div', class_ = 'cb-col cb-col-50 cb-team-item cb-lst-itm cb-team-rght-item'):   \n",
    "    team_name = team_data.text.strip()\n",
    "    team_link = 'https://www.cricbuzz.com' + team_data.find('a').get('href') + '/players'\n",
    "    data.append([team_name, team_link])\n",
    "    \n",
    "df = pd.DataFrame(data, columns = ['name','link'])\n",
    "df.to_csv('teams.csv',index = False)\n",
    "print('teams.csv is created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e64de",
   "metadata": {},
   "source": [
    "### 2. Scraping Players Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f26ee260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 37/37 [00:17<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "players.csv is created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "players_data = []\n",
    "\n",
    "for team_name, team_link in tqdm(data):           # Going through each team\n",
    "    soup = BeautifulSoup(requests.get(team_link).content, 'html.parser')\n",
    "    \n",
    "    for player in soup.find('div', class_ = 'cb-col-67 cb-col cb-left cb-top-zero').find_all('a'): # Players of Team\n",
    "        player_name = player.text.strip()\n",
    "        player_link = 'https://www.cricbuzz.com' + player.get('href').strip()\n",
    "        players_data.append([player_name, team_name, player_link])\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame(players_data, columns = ['player','team','link'])\n",
    "df.to_csv('players.csv', index = False)\n",
    "print('players.csv is created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902458fd",
   "metadata": {},
   "source": [
    "### 3. Scraping  Players Data in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c85a9723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 453/453 [05:50<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "final_data = []\n",
    "\n",
    "for name, team, link in tqdm(players_data):\n",
    "\n",
    "    details = []\n",
    "    \n",
    "    details.append(name)\n",
    "    details.append(team)\n",
    "    \n",
    "    soup = BeautifulSoup(requests.get(link).content,'html.parser')\n",
    "    \n",
    "    try:\n",
    "        # Basic Information\n",
    "        basic_info = [sp.text.strip() for sp in soup.find('div', class_ = 'cb-col cb-col-33 text-black').find_all('div', class_ = 'cb-col cb-col-60 cb-lst-itm-sm')[:3]]\n",
    "        details += basic_info\n",
    "\n",
    "\n",
    "        # Data of Height\n",
    "        height = soup.find('div', class_ = 'cb-col cb-col-33 text-black').find('div', class_ = 'cb-col cb-col-60').text.strip()\n",
    "        details.append(height)\n",
    "\n",
    "        # Ranks\n",
    "        ranks = [sp.text.strip() for sp in soup.find('div', class_ = 'cb-col cb-col-33 text-black').find_all('div', class_ = 'cb-col cb-col-25 cb-plyr-rank text-right')]\n",
    "        details += ranks\n",
    "\n",
    "\n",
    "        details.append(link)    \n",
    "        final_data.append(details)\n",
    "        \n",
    "    except:\n",
    "        details = []\n",
    "    \n",
    "df = pd.DataFrame(final_data, columns = ['name','team','date_of_birth','palce_of_birth','role','height',\n",
    "                                         'test_bat_rank','odi_bat_rank','t20_bat_rank',\n",
    "                                         'test_bow_rank','odi_bow_rank','t20_bow_rank',\n",
    "                                         'link'])\n",
    "\n",
    "df.to_csv('cricket.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
